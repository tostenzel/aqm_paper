The research questions are, first, how reliable is Claassen's method?, and second, can we improve his approach? In the following sections, I explain the changes that we will introduce to the model and the specific quantity of interest (QoI) that we will test for its robustness towards this changes.

## Hyperparameter Selection

Our first goal is to analyze the sensitivity of the MH algorithm towards changes in its hyperparameters. The main options are the models, the priors, the proposal distribution, the number of iterations, the warmup length, and the number of chains. @Claassen2019estimating already tests different models and priors. Therefore, we concern with the other options. Testing different proposal distribution poses the following challenge: MCMC estimation of complex models with sparse data is computationally costly. For instance, one run with the default settings in @Claassen2019estimating takes 50 minutes on my machine^[I use an Intel Core i7-8550U CPU with 8 1.80GHz cores]. Therefore, it is important to use highly optimized libraries such as STAN or JAGS. However, these libraries each implement only one preferred proposal method. STAN uses a proposal distribution based on Hamiltonian dynamics (@Stan), whereas JAGS uses Gibbs sampling (@Plummer2003). The main idea of Gibbs sampling is to iteratively sample from the conditional distribution $p(\Theta|X, \Theta_{-d})$ where $\Theta_{-d}$ is $\Theta$ without the $d$th parameter and to use an acceptance probability equal one. Learning and especially extending these frameworks for additional proposal distributions requires a substantial amount of time and is beyond the scope of this work. Therefore, we will not vary the proposal distributions but follow Claassen's choice STAN. Taken together, we restrict ourselves to analyzing the effect of changes in the remaining hyperparameters: the number of iterations, the warmup length, and the number of chains. Our second goal is to analyze the effect of adding three additional years, namely 2015-2017, to the dataset. Claassen uses this data in his two most recent publications on PSD but not in his first. In the following analysis the default chain number is four and the default warmup is half the chain length (as used by Claassen and as recommend by STAN) unless stated differently. 


## Quantity of Interest Selection

The most illustrative choice would be statistical coefficients that represent the main results in @Claassen2020mood and @Claassen2020support and the associated substantial effects. This choice, however, has the two following disadvantages. First, because these statistical parameters are estimated from simulated data, they are influenced by randomness from the data simulation that we cannot distinguish from the estimation-inherent randomness. Second, the additional steps of simulating data and estimating statistical models would potentially double the computation time. For these reasons, I choose the latent, dynamic, country-specific PSD $\theta$ as the quantity of interest. This has the disadvantage that we cannot evaluate how Claassen's substantial results would change. For instance, we cannot say whether his main coefficients stay equal, become insignificant or change in direction. We can only determine whether the intermediate results, the PSD coefficients, show changes in magnitude that suggest a too strong dependence on the hyperparameter choices, or not.


```{r prepare-stan-estimation}
# for timing use:
# start_time <- Sys.time()
# end_time <- Sys.time()
# time_passed = start_time - end_time

# parallelization
options(mc.cores = parallel::detectCores())
#allows to automatically save a bare version of a compiled Stan program to the hard disk so that it does not need to be recompiled (unless you change it).
rstan_options(auto_write = TRUE)

# prepare data for stan
n.items = length(unique(df2$Item))
n.cntrys = length(unique(df2$Country))
n.yrs = 2017-year0
n.proj = length(unique(df2$Project))
n.resp = dim(df2)[1]
n.itm.cnt = length(unique(df2$ItemCnt))
n.cntry.yrs = n.cntrys * n.yrs
n.yr.proj.cnt = length(unique(df2$YrProjCnt))
cntrys = as.numeric(factor(df2$Country))
cnt.names = as.character(sort(unique(df2$Country)))
cnt.code = as.character(df2[match(cnt.names, df2$Country), "CAbb"])
cnt.code[83] = "MNE"
cnt.code[106] = "SRB"
items = as.numeric(factor(df2$Item))
yrs = df2$Year
projs = as.numeric(factor(df2$Project))
itm.cnts = as.numeric(factor(df2$ItemCnt))

# specify data for stan
dat.1 = list(N=n.resp, K=n.items, T=n.yrs, J=n.cntrys, jj=cntrys, tt=yrs, kk=items, 
             x=df2$Response, samp=df2$Sample)
dat.2 = list(N=n.resp, K=n.items, T=n.yrs, J=n.cntrys, P=n.itm.cnt, jj=cntrys, tt=yrs, 
             pp=itm.cnts, kk=items, x=df2$Response, samp=df2$Sample)
sapply(dat.2, summary)

# pars
pars.5 = c("mu_lambda","sigma_lambda","sigma_delta","sigma_theta","phi","lambda","delta","theta",
            "x_pred","log_lik")

# hyperparams
n.chn = 4
n.thin = 1
iter1 =  seq(from=10, to=460, by=30)
iter2 =  seq(from=500, to=1000, by=100)
itergrid = c(iter1, iter2)
```

```{r generate-data-from-different-niter, eval=FALSE}
# takes too much time - don't execute for superficial replication
for(n.iter in itergrid){
  n.warm = n.iter/2

  # best model from PA article (model 5)
  stan.mod.5 = stan(file='model5.stan', data=dat.2, pars=pars.5, 
                     iter=n.iter, warmup=n.warm, chains=n.chn, thin=n.thin, 
                     control=list(adapt_delta=0.99, stepsize=0.02, max_treedepth=11), seed=n.iter)
  
  #### Extract theta estimates
  theta.out = rstan::extract(stan.mod.5, pars = c("theta"))[[1]]
  theta.std = (theta.out - mean(as.vector(theta.out))) / sd(as.vector(theta.out)) # standardize
  theta.out.t = apply( theta.std, 1, function(x) t(x) )
  theta.out.df = data.frame(Country=rep(cnt.names, length.out=n.cntrys*30), 
                            ISO_code=rep(cnt.code, length.out=n.cntrys*30),
                            Year=rep(1988:2017, each=n.cntrys), theta.out.t)
  theta.pe = theta.out.df[,1:3]
  theta.dim = dim(theta.out.df)[2]
  theta.pe$SupDem_trim = apply(theta.out.df[,4:theta.dim], 1, mean)
  
  first.yr = data.frame(Country=levels(df2$Country),
                        First_yr = as.vector(by(df2, df2$Country, function(x) min(as.numeric(x$Year))+1987)))
  
  theta.pe = merge(theta.pe, first.yr, by="Country", all.x=TRUE)
  cnts = theta.pe[theta.pe$Year==2008, "Country"]
  frst.yr = theta.pe[theta.pe$Year==2008, "First_yr"]
  for(i in 1:length(cnts)) {
    theta.pe[theta.pe$Country==cnts[i] & theta.pe$Year<frst.yr[i], "SupDem_trim"] = NA
  }
  path = paste("output/niter/stan_est_sup_dem_m5_run", as.character(n.iter), ".csv", sep="")
  write.csv(theta.pe, path , row.names=FALSE)
  
}
```


```{r merge-iter-data}
df_iter = read.csv("output/niter/stan_est_sup_dem_m5_run10.csv")
names(df_iter)[names(df_iter) == 'SupDem_trim'] <- 'theta10'

for(n.iter in itergrid[-1]){#without first iteration
  temp_str = paste("output/niter/stan_est_sup_dem_m5_run", as.character(n.iter), ".csv", sep="")
  df_temp = read.csv(temp_str)
  temp_str = paste("theta", as.character(n.iter), sep="")
  df_iter[temp_str] = df_temp$SupDem_trim
}

df_iter$First_yr = NULL
df_iter$ISO_code = NULL

cnt.obs.years = cnt.obs.years[cnt.obs.years > 1]
bottom3_data_countries = names(sort(cnt.obs.years)[1:3])
top3_data_countries = names(sort(cnt.obs.years)[(length(cnt.obs.years)-2):(length(cnt.obs.years))])
# select countries
Austria = as.numeric(df_iter[df_iter$Country=="Austria" & df_iter$Year==2008, ][, c(3:ncol(df_iter))])
Azerbaijan = as.numeric(df_iter[df_iter$Country=="Azerbaijan" & df_iter$Year==2008, ][, c(3:ncol(df_iter))])
Bahrain =  as.numeric(df_iter[df_iter$Country=="Bahrain" & df_iter$Year==2008, ][, c(3:ncol(df_iter))])
Uruguay = as.numeric(df_iter[df_iter$Country=="Uruguay" & df_iter$Year==2008, ][, c(3:ncol(df_iter))])
Venezuela = as.numeric(df_iter[df_iter$Country=="Venezuela" & df_iter$Year==2008, ][, c(3:ncol(df_iter))])
Argentina = as.numeric(df_iter[df_iter$Country=="Argentina" & df_iter$Year==2008, ][, c(3:ncol(df_iter))])

df = data.frame(itergrid, Austria, Azerbaijan, Bahrain, Uruguay, Venezuela, Argentina)
```

## The Effect of Number of Iterations

Figure \@ref(fig:niter) shows latent PSD for year 2008 in two sets of countries. The dots represent the data points.^[Although it would be interesting to see confidence intervals in this graph, I do not report them as this would require me to re-run three days of computations. It can be expected that the uncertainty is proportional to the first derivative of each line. This means, it vanishes quickly but even more for the countries with more data.] The set shown by the warm-colored lines are the countries with the smallest number of years with available data: Austria, Azerbaijan and Bahrain. These three countries have only two years with at least two available PSD items. The cold-colored lines represent the countries with the highest data availability: Uruguay (22), Venezuela (22), and Argentina (23). The vertical gray line shows the number of iterations that @Claassen2019estimating uses. We can make two observations: First, the parameter estimates have converged to a reasonable degree around 150. This is only a fraction of what @Claassen2019estimating chooses. Second, the parameter estimates for countries with less available data show a larger initial error and slower convergence. The main takeaway is that we can save a lot of computational time in our estimation by using much shorter Markov chains. For the next simulations, I use the reduced iteration number.

```{r create-plot-niter, label="niter", include=TRUE, echo=FALSE, fig.cap="The effect of changes in the number of MH iterations on latent PSD in 2008.", warning=FALSE, fig.pos="H"}
df_plot <- df %>%
  gather(key = "Country", value = "theta", -itergrid)
#head(df_plot)

# Visualization
# Reorder following the value of another column:
df_plot %>%
  mutate(Country = fct_relevel(Country,"Austria", "Azerbaijan", "Bahrain", "Uruguay", "Venezuela", "Argentina")) %>%
  ggplot(aes(x = itergrid, y = theta, colour = Country))  + geom_line()  + geom_point() + xlab("Iteration") + ylab(TeX("$\\hat{\\theta}_{2008}$")) + grids(linetype = "dashed") + scale_colour_brewer(palette="RdYlBu") + geom_vline(aes(xintercept=500), linetype="dashed", color = "black", show.legend = FALSE) + annotate("text", 625, 1.45 , vjust = -1, label = "Claassen (2019)", colour="black") + scale_x_continuous(labels= c(10, 250, 500, 750, 1000)) 

```


```{r generate-data-from-different-warmups, eval=FALSE}
# BUG: Seed for 0 is always 0 -> no variation

warm_grid = seq(0, 9, 1)

# number of repititions for distr
rep_grid = seq(1,10,1)

n.chn = 4
n.thin = 1
n.iter = 150

#for(warm in warm_grid){
n.warm = floor(n.iter* warm/10) # throw away warm_grid * 10 percent
n.warm = 0
for (rep in rep_grid){

  # best model from PA article (model 5)
  stan.mod.5 = stan(file='model5.stan', data=dat.2, pars=pars.5, 
                     iter=n.iter, warmup=n.warm, chains=n.chn, thin=n.thin, 
                     control=list(adapt_delta=0.99, stepsize=0.02, max_treedepth=11), seed=(n.warm + 10)*rep)
  
  #### Extract theta estimates
  theta.out = rstan::extract(stan.mod.5, pars = c("theta"))[[1]]
  theta.std = (theta.out - mean(as.vector(theta.out))) / sd(as.vector(theta.out)) # standardize
  theta.out.t = apply( theta.std, 1, function(x) t(x) )
  theta.out.df = data.frame(Country=rep(cnt.names, length.out=n.cntrys*30), 
                            ISO_code=rep(cnt.code, length.out=n.cntrys*30),
                            Year=rep(1988:2017, each=n.cntrys), theta.out.t)
  theta.pe = theta.out.df[,1:3]
  theta.dim = dim(theta.out.df)[2]
  theta.pe$SupDem_trim = apply(theta.out.df[,4:theta.dim], 1, mean)
  
  first.yr = data.frame(Country=levels(df2$Country),
                        First_yr = as.vector(by(df2, df2$Country, function(x) min(as.numeric(x$Year))+1987)))
  
  theta.pe = merge(theta.pe, first.yr, by="Country", all.x=TRUE)
  cnts = theta.pe[theta.pe$Year==2008, "Country"]
  frst.yr = theta.pe[theta.pe$Year==2008, "First_yr"]
  for(i in 1:length(cnts)) {
    theta.pe[theta.pe$Country==cnts[i] & theta.pe$Year<frst.yr[i], "SupDem_trim"] = NA
  }
  path = paste("output/warm/stan_est_sup_dem_m5_warm", as.character(n.warm), "_rep", as.character(rep), ".csv", sep="")
  write.csv(theta.pe, path , row.names=FALSE)
#  }
}
```

```{r extract-QoI-benchmark}
df_bl = read.csv("output/niter/stan_est_sup_dem_m5_run1000.csv")
names(df_bl)[names(df_bl) == 'SupDem_trim'] <- 'theta0'
bl = df_bl[df_bl$Year == 2008 & df_bl$Country == "United States of America","theta0"]
```


```{r merge-data-warmup}
warm_grid = seq(0, 9, 1)

# number of repititions for distr
rep_grid = seq(1,10,1)

n.iter = 150
df_warm= read.csv("output/warm/stan_est_sup_dem_m5_warm0_rep1.csv")
names(df_warm)[names(df_warm) == 'SupDem_trim'] <- 'theta0'
theta0 = c(df_warm[df_warm$Year == 2008 & df_warm$Country == "United States of America","theta0"], NA, NA, NA, NA, NA, NA, NA, NA, NA)
df_warm <- data.frame(theta0)

for(warm in warm_grid){
  n.warm = floor(n.iter* warm/10) # throw away warm_grid * 10 percent
  theta_str = paste(as.character(n.warm), sep="")
  df_warm[theta_str] = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)

  for (rep in rep_grid){
    
    path_str = paste("output/warm/stan_est_sup_dem_m5_warm", as.character(n.warm), "_rep", as.character(rep), ".csv", sep="")
    df_temp = read.csv(path_str)
    df_warm[theta_str][rep, ] = df_temp[df_temp$Year == 2008 & df_temp$Country == "United States of America",'SupDem_trim']
  }   
}
df_warm$theta0 = NULL
```

## The Effect of Warmup Length

Figure \@ref(fig:warmup) shows the distributions of 10 latent PSD estimates obtained from MCMC samples with warmup lengths between 0 and 135 (or 0 to 90 percent chain length). Note that the warmup period is part of the actual MCMC sample and not a separate prior sample. The red horizontal line shows the parameter estimate obtained from a run with 1000 iterations. We make two observations: First, the default warmup of half the chain indeed achieves the results closest to the benchmark. This makes sense because, on the one hand, a too small warmup implies that the chains has less iterations to converge. On the other hand, a too high warmup leaves a smaller sample with a less stable mean. Second, as can be expected it is worse choosing a too small as choosing a too high warmup in terms of precision.

```{r create-plot-warmup, label="warmup", include=TRUE, echo=FALSE, results='hide',fig.keep='all', fig.cap="The effect of changes in the warmup lengths on latent PSD in 2008 in the US. Three outliers for zero warmup are not shown.", warning=FALSE, fig.pos="H", message=FALSE}
sdf = stack(df_warm)
sdf$Warmup <- as.factor(sdf$ind) # for different color tones
sdf$ind = NULL

grids(axis = c("xy", "x", "y"), color = "grey92", size = NULL, linetype = NULL)

#sdf["theta"] = sdf$ind
ggplot(sdf, aes(x=Warmup, y=values, fill=Warmup)) + 
  geom_boxplot(binaxis='y', stackdir='center', show.legend = FALSE) + scale_x_discrete(labels= floor(n.iter* warm_grid/10)) + xlab("Warmup") + ylab(TeX("$\\hat{\\theta}_{USA, 2008}$")) + scale_fill_brewer(palette="Blues") + grids(linetype = "dashed") + theme(legend.position="none") + geom_hline(aes(yintercept=bl), linetype="dashed", color = "red", show.legend = TRUE) + annotate("text", 1.3, bl-0.025, vjust = -1, label = "Benchmark", colour="red") + ylim(0.1, 0.9)

```





```{r generate-data-from-different-nchains, eval=FALSE}
chain_grid = seq(1, 8, 1)

# number of repititions for distr
rep_grid = seq(1,3,1)

n.chn = 4
n.thin = 1
n.iter = 150
n.warm = floor(n.iter/2)

for(n.chain in chain_grid){
  for (rep in rep_grid){
  
    # best model from PA article (model 5)
    stan.mod.5 = stan(file='model5.stan', data=dat.2, pars=pars.5, 
                       iter=n.iter, warmup=n.warm, chains=n.chn, thin=n.thin, 
                       control=list(adapt_delta=0.99, stepsize=0.02, max_treedepth=11), seed=n.chain*rep)
    
    #### Extract theta estimates
    theta.out = rstan::extract(stan.mod.5, pars = c("theta"))[[1]]
    theta.std = (theta.out - mean(as.vector(theta.out))) / sd(as.vector(theta.out)) # standardize
    theta.out.t = apply( theta.std, 1, function(x) t(x) )
    theta.out.df = data.frame(Country=rep(cnt.names, length.out=n.cntrys*30), 
                              ISO_code=rep(cnt.code, length.out=n.cntrys*30),
                              Year=rep(1988:2017, each=n.cntrys), theta.out.t)
    theta.pe = theta.out.df[,1:3]
    theta.dim = dim(theta.out.df)[2]
    theta.pe$SupDem_trim = apply(theta.out.df[,4:theta.dim], 1, mean)
    
    first.yr = data.frame(Country=levels(df2$Country),
                          First_yr = as.vector(by(df2, df2$Country, function(x) min(as.numeric(x$Year))+1987)))
    
    theta.pe = merge(theta.pe, first.yr, by="Country", all.x=TRUE)
    cnts = theta.pe[theta.pe$Year==2008, "Country"]
    frst.yr = theta.pe[theta.pe$Year==2008, "First_yr"]
    for(i in 1:length(cnts)) {
      theta.pe[theta.pe$Country==cnts[i] & theta.pe$Year<frst.yr[i], "SupDem_trim"] = NA
    }
    path = paste("output/chain/stan_est_sup_dem_m5_chain", as.character(n.chain), "_rep", as.character(rep), ".csv", sep="")
    write.csv(theta.pe, path , row.names=FALSE)
  }
}
```


```{r merge-data-chains}
chain_grid = seq(1, 8, 1)

# number of repititions for distr
rep_grid = seq(1,3,1)

n.iter = 150

df_chain= read.csv("output/chain/stan_est_sup_dem_m5_chain1_rep1.csv")
names(df_chain)[names(df_chain) == 'SupDem_trim'] <- 'theta1'
theta1 = c(df_chain[df_chain$Year == 2008 & df_chain$Country == "United States of America","theta1"], NA, NA)
df_chain <- data.frame(theta1)

for(chain in chain_grid){
  theta_str = paste("theta", as.character(chain), sep="")
  df_chain[theta_str] = c(NA, NA, NA)

  for (rep in rep_grid){
    
    path_str = paste("output/chain/stan_est_sup_dem_m5_chain", as.character(chain), "_rep", as.character(rep), ".csv", sep="")
    df_temp = read.csv(path_str)
    df_chain[theta_str][rep, ] = df_temp[df_temp$Year == 2008 & df_temp$Country == "United States of America",'SupDem_trim']
  }   
}

```

## The Effect of Number of Chains

Figure \@ref(fig:chain) shows the distribution of three latent PSD estimates for MCMC samples with one to eight chains. In this experiment the number of chains has no significant effect on the latent PSD estimates. The higher the number of chains, the smaller is each single chain. Hence, there should be a configuration where the smaller subsamples have just not converged to the posterior distribution. There, we could potentially find some differences resulting from variation in the number of chains. This point, however, is also dependent on the warmup and the number of iterations. Unfortunately, ablating these three hyperparameters at once to find this point would take too many computations for this paper.

```{r create-plot-nchains, label="chain", include=TRUE, echo=FALSE, results='hide',fig.keep='all', fig.cap="The effect of changes in the number of chains on latent PSD in 2008 in the US.", warning=FALSE, fig.pos="H", message=FALSE}
sdf = stack(df_chain)
sdf$Chain <- as.factor(sdf$ind) # for different color tones
sdf$ind = NULL

grids(axis = c("xy", "x", "y"), color = "grey92", size = NULL, linetype = NULL)

#sdf["theta"] = sdf$ind
ggplot(sdf, aes(x=Chain, y=values, fill=Chain)) + 
  geom_dotplot(binaxis='y', stackdir='center') + scale_x_discrete(labels= seq(1, 8, 1)) + xlab("Number of chains") + ylab(TeX("$\\hat{\\theta}_{USA, 2008}$")) + scale_fill_brewer(palette="Blues") + grids(linetype = "dashed") + theme(legend.position="none") + ylim(bl-0.05, bl+0.05)+ geom_hline(yintercept=bl, linetype="dashed", color = "red") + geom_hline(aes(yintercept=bl), linetype="dashed", color = "red", show.legend = TRUE) + annotate("text", 1.2, bl+0.0, vjust = -1, label = "Benchmark", colour="red")
```


## The Effect of Three Additional Years of Data

Figure \@ref(fig:data) shows the effect that adding years 2016-2018 to the dataset from 1998 - 2015 has for the USA, China and South Africa. The pale lines show the QoI obtained from the smaller dataset and the strong lines the ones obtained from the updated data. We observe two points. First, all updated QoIs are more extreme in the sense that they have a larger difference from zero. However, the ranking between countries over time remains the same. Therefore, the more data is added, the better we can differentiate between latent PSD in different countries. Second, there is a pronounced shift in directions between the old estimates and the updated estimates for China in the period from 2006 to 2007. It is remarkable that we can see such a difference although the added data is roughly 10 years later. Moreover, it is realistic to expect that we could find such anomalies for a number of other countries, too. Such shifts like that can potentially have a strong impact on the data simulations and question the reliability of Claassen's results and his method.

```{r create-plot-data, label="data", include=TRUE, echo=FALSE, fig.cap="The effect of adding data from 2016 to 2018 on latent PSD in the US, China and South Africa.", warning=FALSE, fig.pos="H"}

years = c(2005, 2006, 2007, 2008, 2009)
us_old = c(0.36, 0.29, 0.21, 0.18, 0.22)
sa_old = c(-0.26, -0.32, -0.38, -0.38, -0.38)
china_old = c(-0.52, -0.75, -0.74, -0.67, -0.61)
 
df = read.csv("output/niter/stan_est_sup_dem_m5_run1000.csv")
names(df)[names(df) == 'SupDem_trim'] <- 'theta'

df$First_yr = NULL
df$ISO_code = NULL

# as.numeric
df_temp = df[which(df["Country"]=="United States of America" & 2004 < df["Year"] & df["Year"] < 2010), ]
df_temp <-df_temp[order(df_temp$Year),]
us = df_temp[["theta"]]

df_temp = df[which(df["Country"]=="South Africa" & 2004 < df["Year"] & df["Year"] < 2010), ]
df_temp <-df_temp[order(df_temp$Year),]
sa = df_temp[["theta"]]

df_temp = df[which(df["Country"]=="China" & 2004 < df["Year"] & df["Year"] < 2010), ]
df_temp <-df_temp[order(df_temp$Year),]
china = df_temp[["theta"]]

df = data.frame(years, us_old, us, china_old, china, sa_old, sa)

df_plot <- df %>%
  gather(key = "Country", value = "theta", -years)
#head(df_plot)

# Visualization
# Reorder following the value of another column:
df_plot %>%
  mutate(Country = fct_relevel(Country, "us_old", "us", "china_old", "china", "sa_old", "sa")) %>%
  ggplot(aes(x = years, y = theta, colour = Country)) + grids(linetype = "dashed")  + geom_line()  + geom_point() + xlab("Year") + ylab(TeX("$\\hat{\\theta}$")) + scale_colour_brewer(palette="Paired", labels = c(TeX("$US_{-'15}$"), TeX("$US_{-'18}$"), TeX("$China_{-'15}$"), TeX("$China_{-'18}$"), TeX("$SA_{-'15}$"), TeX("$SA_{-'18}$")))
```
