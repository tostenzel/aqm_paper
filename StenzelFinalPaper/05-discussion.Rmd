In the previous section we find three results: First, updating the dataset from 1997 to 2015 by three additional years lead to noticable changes in latent PSD for one of three cases. In particular the direction of the change in latent PSD has changed for 2/5 of the observed period. If this finding is representative, the substantial results of Claassen's subsequent analyses differ significantly between one dataset and a dataset with only 3 year more. This raises the question whether the two main results (increases in democracy have a negative effect on PSD and PSD is positively associated with subsequent change in democracy regardless of the initial level of democracy) are reliable. It would be important to secure this preliminary finding in future research. In particular, this requires to re-compute the substantial findings in @Claassen2020mood and @Claassen2020support based on the smaller dataset and compare those with the reported findings. If the results differ too much this could mean that Claassen's model overfits the data and that it could indeed be more appropriate to research smaller panels, i.e. less country and/or years, until sufficient data is available (as was done in the research previous to Claassen's work). In general, it is better to prioritize reliability over generalizability.

Second, Claassen's estimation is very inefficient. The reason is not the method as such but simply that he chooses to run much more iterations than necessary. Instead of 500 iterations it is sufficient to use only less than 150. On my machine, this reduces the estimation time from 50 to 20 minutes (by 60 $\%$). Therefore, it would be a rewarding investment to try models with less iterations. This has two advantages: First, more robustness or sensitivity tests can be done with the same resources. Second, it is easier for others to replicate the results and to build on previous work.^[As Claassen notes in the readme file of his replication directory the runtime for his computations in the estimation section is 12 hours in his setup.] However, instead of looking at convergence plots for a small subset of important QoIs in order to find the most efficient number of iterations as done here, I suggest a different procedure for future research: if a satisfying estimation setup is found, re-run the procedure with a set of smaller iteration numbers. After each step compute either the convergence diagnostic by @Geweke1992 or by @Brooks1998. The first measure requires only one MCMC chain. It divides the chain less the warmup into multiple partitions and tests whether they are similar enough to reject the hypothesis that the chain has not converged. The second measure requires multiple chains and tests for differences between whole chains and within parts of single chains. This approach is more systematic and scales better to multiple QoIs compared to the approach used here.

Third, the number of chains does not have an effect and the warmup length is ideal at the value recommended by STAN. The takeaway here is that it does not add much value to look at changes to these hyperparameters from the defaults that are recommended by experts as long as the posterior sample converges. 
