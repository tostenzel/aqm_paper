
Second, Claassen's estimation is very inefficient. The reason is not the method as such but simply that he chooses to run much more iterations than necessary. Instead of 500 iterations it is sufficient to use only less than 150. On my machine, this reduces the estimation time from 50 to 20 minutes (by 60 $\%$). Therefore, it would be a rewarding investment to try models with less iterations. This has two advantages: First, more robustness or sensitivity tests can be done with the same resources. Second, it is easier for others to replicate the results and to build on previous work.^[As Claassen notes in the readme file of his replication directory the runtime for his computations in the estimation section is 12 hours in his setup.] However, instead of looking at convergence plots for a small subset of important QoIs in order to find the most efficient number of iterations as done here, I suggest a different procedure for future research: if a satisfying estimation setup is found, re-run the procedure with a set of smaller iteration numbers. After each step compute either the convergence diagnostic by @Geweke1992 or by @Brooks1998. The first measure requires only one MCMC chain. It divides the chain less the warmup into multiple partitions and tests whether they are similar enough to reject the hypothesis that the chain has not converged. The second measure requires multiple chains and tests for differences between whole chains and within parts of single chains. This approach is more systematic and scales better to multiple QoIs compared to the approach used here.

Third, the number of chains does not have an effect and the warmup length is ideal at the value recommended by STAN. The takeaway here is that it does not add much value to look at changes to these hyperparameters from the defaults that are recommended by experts as long as the posterior sample converges. 
