In the previous section we find three results: First, the number of chains does not have an effect and the warmup length is ideal at the value recommended by STAN. The takeaway here is that it does not add much value to look at changes to these hyperparameters from the defaults that are recommended by experts on MCMC methods as long as the posterior sample has converged. Second, updating the dataset from 1997 to 2015 by three additional years lead to noticable changes in latent PSD for one of three cases. In particular the direction of the change in latent PSD has changed for 2/5 of the observed period. If this finding is representative, the substantial results of Claassen's subsequent analyses differ significantly between one dataset and a dataset with only 3 year more. It would be interesting to see the findings in @Claassen2020mood and @Claassen2020support based on the smaller dataset and compare those with the reported findings. If the results differ too much this could mean that Claassen's model does not capture the data sufficiently and that it could indeed be more appropriate to research smaller panels, i.e. less country and/or years, until sufficient data is available. Third, Claassen's estimation is very inefficient. The reason is not the method as such but simply that he chooses to run much more iterations than required. Instead of 500 iterations less than 150 are actually sufficient for convergence. On my machine, this reduces the estimation time from 50 to 20 minutes or 60 $\%$. Therefore, it would have been a good investment to try models with less iterations. This has two advantages: First, more robustness or sensitivity tests can be done to secure the results. Second, it is easier for others to replicate the results and to build on Claassen's work in own research.^[As Claassen notes in the Readme file of his replication directory the estimation runtime is 12 hours in his setup.] However, instead of looking at convergence plots for a small subset of important QoIs to find the most efficient number of iterations as done here, I suggest the a different procedure: If a satisfying estimation setup is found, re-run it with a set of smaller iteration numbers. After each step compute either the convergence diagostic by @Geweke1992 or by @Brooks1998. The first measure requires only one MCMC chain. It divides the chain less the warmup into multiple partitions and tests whether they are similar enough to reject the hypothesis that the chain has not converged. The second measures requires multiple chains and tests for differences between chains and also within each single chain. This approach is more systematic and scales better to many QoIs then the approach used here
.