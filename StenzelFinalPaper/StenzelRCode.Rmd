---
title: "On Using the Metropolis-Hastings Algorithm for Data Imputation"
author: "Tobias Stenzel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
header-includes:
   - \usepackage{tikz, float, caption, amsthm}
   - \floatplacement{figure}{H}
   #- \newtheorem{definition}{Definition}
   - \interfootnotelinepenalty=10000 # no multi-page footnotes
output:
  bookdown::pdf_document2:
    template: template.tex
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
    keep_tex: yes
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
bibliography: bibliography.bib
csl-hanging-indent: yes
fontsize: 12pt 
linestretch: 2.0 # adjust for line spacing 
geometry: left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm
classoption:
- a4paper
- oneside
lang: en-EN
numbersections: yes
csquotes: yes
type: Final Paper for Course
course: Advanced Quantitative Methods in Political Science
subtitle: ''
address: ''
email: tobias.stenzel@students.uni-mannheim.de
phone: ''
examiner: Prof. Thomas Gschwend, Ph.D.
chair: ''
mp: 0.55
ID: ''
# STRONGLY BIASED wordcount: '*Wordcount excluding References: `r unima::count_words(knitr::current_input())`*' 
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/american-political-science-association.csl
editor_options:
  markdown:
    wrap: sentence
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      include=FALSE,
                      fig.path = "figs/",
                      out.width="\\textwidth"
                      )

p_needed <- c(# all packages you need to install here
  "knitr",
  "bookdown", # referencing figures and tables by label
  "remotes",
  "ggplot2",
  "stargazer",
  "dplyr", # to compute leads values
  "tidyr", # for dropping nans
  "GGally", # correlation matrix style plots
  "gridExtra", # grid for multiple plots
  "cowplot",
  "MASS", # glm
  "janitor", # fd plot over range
  #"countreg", # rootogram
  "ggeasy", # remove axes
  "latex2exp", # latex legend labels
  "ggridges"
  )


# installs only the required packages 
lapply(p_needed[!(p_needed %in% rownames(installed.packages()))], install.packages)
lapply(p_needed, library, character.only = TRUE)

# separately install for the correct wordcount package 
if (!"unima" %in% rownames(installed.packages())){
  remotes::install_github("vktrsmnv/unima-template", upgrade = "never", dependencies = TRUE)
  }

# this allows you to add notes to figures with a simple chunk option
# you only need to add "notes="text" as a chunk option; 
# the notes will only appear in PDF output
hook_chunk = knit_hooks$get('chunk')
knit_hooks$set(chunk = function(x, options) {
  txt = hook_chunk(x, options)
  # add chunk option 'description' which adds \Description{...} to figures
  if (!is.null(options$notes)) {
    latex_include <- paste0("\\\\vspace\\{0.5cm\\} \\\\\\footnotesize\\{\\\\textit\\{Notes: \\}", options$notes, "\\} \\1")
    gsub('(\\\\end\\{figure\\})', latex_include, txt) 
  } else {
    return(txt)  # pass to default hook
  }
})
if (knitr::is_latex_output()) knitr::knit_hooks$set(plot = knitr::hook_plot_tex)

# This is an option for stargazer tables
# It automatically adapts the output to html or latex,
# depending on whether we want a html or pdf file
stargazer_opt <- ifelse(knitr::is_latex_output(), "latex", "html")

# This ensures that if the file is knitted to HTML,
# significance notes are depicted correctly 
if (stargazer_opt == "html"){
  fargs <- formals(stargazer)
  fargs$notes.append = FALSE
  fargs$notes = c("<em>&#42;p&lt;0.1;&#42;&#42;p&lt;0.05;&#42;&#42;&#42;p&lt;0.01</em>")
  formals(stargazer) <- fargs
}

# only relevant for ggplot2 plotting
# setting a global ggplot theme for the entire document to avoid 
# setting this individually for each plot 
theme_set(theme_classic() + # start with classic theme 
  theme(
    plot.background = element_blank(),# remove all background 
    plot.title.position = "plot", # move the plot title start slightly 
    legend.position = "bottom" # by default, put legend on the bottom
  ))

set.seed(2021)

```
# Background

## Introduction

The Metropolis-Hastings (MH) algorithm is a method for sampling data points from a probability distribution. It places among the top 10 algorithms with the greatest influence on science and engineering in the 20th century (insert reference). The MH algorithm belongs to the class of Markov chain Monte Carlo (MCMC) methods. In this section, I assume prior knowledge on Monte Carlo sampling. However, I will explain the basics of Markov Chains. This section is structured as follows. First, I motivate the usage of the MH algorithm. Second, I explain the basics of Markov Chains. Third, I present the algorithm and finally,  I show why it works.

## Motivation

One main application for the MH algorithm is Bayesian inference. Specifically, we want to estimate parameters $\theta$ of some probabilistic model $f$. We have only limited prior knowledge of the distribution of $\theta$ and we have a likelihood sample of $f$. The goal is to estimate the posterior distribution of $\theta$ given all information that we have. In practice, we do not have a formal definition of the likelihood but only observations. Therefore, we can only approximate the posterior by numerical integration. This means, we can sample from the posterior and, for example, compute summary statistics like mean and variance of $\theta$. Claassen (2019, insert reference) assumes a model $f$ parametrized by $\theta$ and uses incomplete panel data as the likelihood. Then, he estimates the parameter distribution from the posterior obtained by the MH algorithm. In a final step, he uses the parameter estimates and samples the missing observations from the probabilistic model $f$.

## Markov Chains

A Markov chain is a stochastic process (over time) with the property that the probability of the realization in the next period depends solely on the realization in the current state and not the complete history. This is called the Markov property. Because discrete Markov chains are much more accessible than their continuous variant, we will look only on the discrete case in this chapter. Formally, the Markov property states that

\begin{equation}
\tag{eq:markov-property}
P(X_{t+1} |X_{t}, X_{t-1}, ..., X_{0}) = P(X_{t+1} |X_{t}).
\end{equation}

Under specific condition, the stochastic process described by a Markov chain converges to a time-invariant probability distribution, i.e. $P(X_{t+k} |X_{t+l}) = P(X_{t} |X_{t-1}) \forall k,l \geq 0, l < k$. The crucial step for understanding the MH is to see how it is able to generate a stable posterior distribution that has converged. Before exploring how the MH algorithm employs these conditions it is thus necessary to understand them conceptually. To this end, we will use the example depicted by the following graph showing the intertemporal transition probabilities between three states:

This transition graph can be summarized by a 3x3 transition matrix T where each element $(i,j)$ represents the probability of moving from state $i$ in period $t$ to state $k$ in period $t+1$, i.e $T_{i,j} = P(X_{t+1}=j | X_t = i)$. For our example, we have

\begin{equation}
\tag{eq:markov-property}
T=
\begin{pmatrix}
0 & 1 & 0\\
0 & 0.1 & 0.9\\
0.6 & 0.4 & 0
\end{pmatrix}\end{equation}

### Limit Distribution

As touched in the previous section, one interesting question can be what the probability of each state $j \in \{1, ..., s\}$ is after infinitely many steps. For this purpose let $\pi (j) = P(X_t = j)$ be the probability of being in state $j$ in period $t$. Of course, the outcome will depend on the initial state $\pi_0$ - or the probabilities of the respective state in $t=0$. Then we can use the law of total probability to calculate the probability of each state for the next period $t=1$ by 

\begin{equation}
\tag{eq:tot-prob}
P(X_1 = j) = \sum_{i=1}^{3} P(X_1 = j | X_0 = i) \pi_0(i).
\end{equation}

I.e., to compute the probability of being in state $j$ in $t=1$, for each initial state $i$, we multiply its probability by the probability of moving from $i$ to state $j$. This is equivalent to $\pi_1 = \pi_0 T$ in vector notation. Further, we can compute the distributions for more distant distributions by repeating the matrix multiplication, e.g, $\pi_2 = \pi_0 T T$, or more general, $\pi_t = \pi_0 T^t$.

We can now define a limit distribution that describes the probability distribution after infinitely many periods as

\begin{equation}
\tag{eq:lim-dist}
\pi_{\infty} = lim_{t \rightarrow \infty} \pi_t = lim_{t \rightarrow \infty} \pi_0 T^t.
\end{equation}

Two important question about a well-defined Markov chain is, (i) does a limit distribution exist?, and (ii), is it unique? In our example, there does not only exist a limit distrubtion with $\pi_{infty} = (0.2, 0.4, 0.4)$, it is even unique regardless of start distribution $\pi_0$!. This means that regardless of the start state, the probability of each state converges to the same number. For the context of the MH algorithm, this is an important property because we always want to compute the same estimates, regardless of the starting values of our simulation. The next sections introduce and simplify conditions that guarantee a unique limit distribution.

### Irreducibility, Periodicity and Stationarity

\begin{definition}
A Markov chain is called irreducible if each state is reachable from any other state in a finite number of steps.
\end{definition}

\begin{figure}[H]
\label{fig:ex1}
\centering

\begin{tikzpicture}[->,shorten >=1pt,auto,node distance=4cm,
                thick,main node/.style={circle,draw,font=\Large\bfseries}]

  \node[main node] (B) {B};
  \node[main node] (A) [below left of=B] {A};
  \node[main node] (C) [below right of=B] {C};

  \path
    (B) edge [loop above] node {0.1} (B)
        edge [bend left] node {0.9} (C)
    (A) edge [bend left] node {1} (B)
    (C) edge [bend left] node {0.6} (A)
        edge [bend left] node {0.4} (B);      
\end{tikzpicture}

\caption{Transition Graph for Markov Chain with 3 states.}

\end{figure}

Figure 2 shows two times our example Markov chain. Obviously, this Markov chain is not irreducible because the initial state impacts all future distributions because starting in one subgraphs sets the probability of reaching states in the other subgraph to zero. Therefore, a Markov Chain is not irreducible if there is no indicrect link between every pair of states.

\begin{figure}[H]
\label{fig:ex2}
\centering

\begin{tikzpicture}[->,shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\Large\bfseries}]

  \node[main node] (B) {B};
  \node[main node] (A) [below left of=B] {A};
  \node[main node] (C) [below right of=B] {C};
  \node[main node] (D) [right of=C]{D};
  \node[main node] (E) [above right of=D] {E};
  \node[main node] (F) [below right of=E] {F};


  \path
    (B) edge [loop above] node {0.1} (B)
        edge [bend left] node {0.9} (C)
    (A) edge [bend left] node {1} (B)
    (C) edge [bend left] node {0.6} (A)
        edge [bend left] node {0.4} (B)
    (E) edge [loop above] node {0.1} (E)
        edge [bend left] node {0.9} (F)
    (D) edge [bend left] node {1} (E)
    (F) edge [bend left] node {0.6} (D)
        edge [bend left] node {0.4} (E); 
\end{tikzpicture}

\caption{Transition Graph for Irreducible Markov Chain.}

\end{figure}



\begin{definition}
A state $i$ has a period k if any return to state $i$ must occur in $k$ time periods. A Markov chain is aperiodic if the period of all its states is 1.
\end{definition}


\begin{figure}[H]
\label{fig:ex3}
\centering

\begin{tikzpicture}[->,shorten >=1pt,auto,node distance=3cm,thick,main node/.style={circle,draw,font=\Large\bfseries}]
  
  
  	\node[main node] (A) {A}; 
  	\node[main node] (B) [right of=A] {B};
  	\node[main node] (C) [below of=A] {C}; 	
  	\node[main node] (D) [below of=B] {D}; 
  

   \path
    (A) edge node {1/3} (B)
    (A) edge node[left] {2/3} (C)
    
    (B) edge node {1} (D)
    
    (C) edge node[below] {1} (D)

    (D) edge node[above] {1} (A);  

  \end{tikzpicture}
  
  \caption{Markov Chain with 3-periodic State A}

\end{figure}




\begin{definition}
$\pi^*$ is the stationary distribution of a Markoc Chain with Transition matrix T if $\pi^* = \pi^* T$ and $\pi^*$ is a probability vector.
\end{definition}



### Basic Limit Theorem

### Reversibility

## The Algorithm

## A short proof

\newpage

# References {.unnumbered}

\singlespacing

::: {#refs}
:::


\clearpage

# Statutory Declaration {.unnumbered}

Hiermit versichere ich, dass diese Arbeit von mir persönlich verfasst ist und dass ich keinerlei fremde Hilfe in Anspruch genommen habe.
Ebenso versichere ich, dass diese Arbeit oder Teile daraus weder von mir selbst noch von anderen als Leistungsnachweise andernorts eingereicht wurden.
Wörtliche oder sinngemäße Übernahmen aus anderen Schriften und Veröffentlichungen in gedruckter oder elektronischer Form sind gekennzeichnet.
Sämtliche Sekundärliteratur und sonstige Quellen sind nachgewiesen und in der Bibliographie aufgeführt.
Das Gleiche gilt für graphische Darstellungen und Bilder sowie für alle Internet-Quellen.
Ich bin ferner damit einverstanden, dass meine Arbeit zum Zwecke eines Plagiatsabgleichs in elektronischer Form anonymisiert versendet und gespeichert werden kann.
Mir ist bekannt, dass von der Korrektur der Arbeit abgesehen werden kann, wenn die Erklärung nicht erteilt wird.

```{=tex}
\SignatureAndDate{}
\renewcommand*{\thepage}{ }
```
\noindent I hereby declare that the paper presented is my own work and that I have not called upon the help of a third party.
In addition, I affirm that neither I nor anybody else has submitted this paper or parts of it to obtain credits elsewhere before.
I have clearly marked and acknowledged all quotations or references that have been taken from the works of other.
All secondary literature and other sources are marked and listed in the bibliography.
The same applies to all charts, diagrams and illustrations as well as to all Internet sources.
Moreover, I consent to my paper being electronically stores and sent anonymously in order to be checked for plagiarism.
I am aware that the paper cannot be evaluated and may be graded "failed" ("nicht ausreichend") if the declaration is not made.

\SignatureAndDateEng{}

<!-- Line below depicts the content that should not be counted in the wordcount -->
<!---TC:ignore--->
